# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/learner.ipynb.

# %% auto 0
__all__ = ['Callback', 'run_cbs', 'to_cpu', 'MetricsCB', 'DeviceCB', 'CancelFitException', 'CancelBatchException',
           'CancelEpochException', 'with_cbs', 'Learner', 'TrainLearner', 'ProgressCB', 'MomentumLearner', 'LR_find',
           'lr_find']

# %% ../nbs/learner.ipynb 1
import math,torch,matplotlib.pyplot as plt
import fastcore.all as fc
from collections.abc import Mapping
from operator import attrgetter
from functools import partial
from copy import copy

from torch import optim
import torch.nn.functional as F

from .conv import *

from fastprogress import progress_bar,master_bar

# %% ../nbs/learner.ipynb 16
class Callback:
    order = 0

# %% ../nbs/learner.ipynb 17
def run_cbs(cbs,call_nm,learn):
    for cb in sorted(cbs, key = attrgetter('order')):
        method = getattr(cb,call_nm,None)
        if method is  not None:
             method(learn)
     
    

# %% ../nbs/learner.ipynb 25
from torcheval.metrics import MulticlassAccuracy,Mean

# %% ../nbs/learner.ipynb 26
def to_cpu(x):
    if isinstance(x, Mapping): return {k:to_cpu(v) for k,v in x.items()}
    if isinstance(x, list): return [to_cpu(v) for v in x]
    if isinstance(x, tuple): return tuple(to_cpu(list(x)))
    res = x.detach().cpu()
    return res.float() if res.dtype==torch.float16 else res

# %% ../nbs/learner.ipynb 27
class MetricsCB(Callback):
    def __init__(self,*ms,**metrics):
        #import pdb; pdb.set_trace()
        for metric in ms: metrics[type(metric).__name__] = metric
        self.metrics = metrics
        self.all_metrics = copy(self.metrics)
        self.all_metrics["loss"] = self.loss = Mean()
        
    def before_fit(self,learn):
        #import pdb;  pdb.set_trace()
        learn.met = 'ffff'
        learn.metrics = self
    def before_epoch(self,learn):
        #import pdb; pdb.set_trace()
        for v in self.all_metrics.values():
            v.reset()
    def after_batch(self,learn):
        #import pdb; pdb.set_trace()
        for o in self.metrics.values():
            o.update(to_cpu(learn.preds),to_cpu(learn.yb))
        self.loss.update(to_cpu(learn.loss),weight = len(learn.xb))
    def _log(self,log):
        print(log)
    def after_epoch(self,learn):
        
        log = {k:v.compute() for k,v in self.all_metrics.items()}
        log["epoch"] = learn.epoch
        log["training"] = learn.model.training
        
        self._log(log)
            
        
        
        

# %% ../nbs/learner.ipynb 32
class DeviceCB(Callback):
    def __init__(self,device = 'cuda'):
        fc.store_attr()
    def before_fit(self,learn):
        learn.hehe = 'lmao'
        if hasattr(learn.model,'to'):
            learn.model.to(self.device)
    def before_batch(self,learn):
        learn.batch = to_device(learn.batch)

# %% ../nbs/learner.ipynb 34
class CancelFitException(Exception): pass
class CancelBatchException(Exception): pass
class CancelEpochException(Exception): pass

# %% ../nbs/learner.ipynb 35
class with_cbs:
    def __init__(self,nm):
        self.nm = nm
    def __call__(self,f):
        def _f(learn,*args,**kwargs):
            try:
                learn.callback(f'before_{self.nm}')
                f(learn,*args,**kwargs)
                learn.callback(f'after_{self.nm}')
            except globals()[f'Cancel{self.nm.title()}Exception']: pass
            finally: learn.callback(f'cleanup_{self.nm}')
        return _f
        
            
            

# %% ../nbs/learner.ipynb 36
class Learner:
    def __init__(self,model,dls,loss_func,epochs,lr,cbs,opt_func = optim.SGD):
        cbs = fc.L(cbs)
        fc.store_attr()
    @with_cbs('batch')
    def one_batch(self):
        self.xb,self.yb = self.batch
        self.predict()
        #import pdb; pdb.set_trace()
        self.callback('after_predict')
        self.calc_loss()
        self.callback('after_loss')
        if self.model.training:
                self.backward()
                self.callback('after_backward')
                self.step()
                self.callback('after_step')
                self.zero_grad()
        
    @with_cbs('epoch')
    def _one_epoch(self):
         for self.n,self.batch in enumerate(self.dl):
            self.one_batch()
            
       
    def one_epoch(self,train):
        self.model.training = train
        self.dl = self.dls.train if train else self.dls.test
        self._one_epoch()
        
    
        n = sum(self.ns)
        #print(self.epoch, self.model.training, sum(self.losses).item()/n , sum(self.acc).item()/n)   
    @with_cbs('fit')
    def _fit(self,train,valid):
        for self.epoch in self.epochs:
                if train : self.one_epoch(True)
                if valid : 
                    with torch.no_grad(): 
                        self.one_epoch(False)
        
        
    def fit(self,epochs=1,lr=0.1,cbs= None,train = True,valid = True):
        #import pdb; pdb.set_trace()
        cbs = fc.L(cbs)
        for cb in cbs: self.cbs.append(cb)
        try:
            self.epochs,self.lr = range(epochs),lr
            self.ns,self.acc,self.losses= [],[],[]
            #self.model.to("cuda")
            self.opt = self.opt_func(self.model.parameters(),self.lr)
            self._fit(train,valid)
        finally:
            for cb in cbs: self.cbs.remove(cb)
    #def __getattr__(self,name):
        #if name in ['predict','calc_loss','backward','step','zero_grad']:
            #return partial(self.callback, name)
        #raise AttributeError(name)   
    def callback(self,call_nm):
        run_cbs(self.cbs,call_nm,self)
    @property
    def training(self): return self.model.training
        

# %% ../nbs/learner.ipynb 37
class TrainLearner(Learner):
    
    def predict(self):
        self.preds = self.model(self.batch[0])
    def calc_loss(self):
        self.loss = self.loss_func(self.preds,self.batch[1])
    def backward(self):
        self.loss.backward()
    def step(self):
        self.opt.step()
    def zero_grad(self):
        self.opt.zero_grad()

# %% ../nbs/learner.ipynb 38
class ProgressCB(Callback):
    order = 1
    def __init__(self, plot=False): self.plot = plot
    def before_fit(self, learn):
        learn.epochs = self.mbar = master_bar(learn.epochs)
        self.first = True
        if hasattr(learn, 'metrics'): learn.metrics._log = self._log
        self.losses = []
        self.val_losses = []

    def _log(self, d):
        #import pdb; pdb.set_trace()
        if self.first:
            self.mbar.write(list(d), table=True)
            self.first = False
        self.mbar.write(list(d.values()), table=True)

    def before_epoch(self, learn): learn.dl = progress_bar(learn.dl, leave=False, parent=self.mbar)
    def after_batch(self, learn):
        learn.dl.comment = f'{learn.loss:.3f}'
        #import pdb; pdb.set_trace()
        if self.plot and hasattr(learn, 'metrics') and learn.training:
            self.losses.append(learn.loss.item())
            if self.val_losses: self.mbar.update_graph([[fc.L.range(self.losses), self.losses],[fc.L.range(learn.epoch).map(lambda x: (x+1)*len(learn.dls.train)), self.val_losses]])
    
    def after_epoch(self, learn): 
        if not learn.training:
            if self.plot and hasattr(learn, 'metrics'): 
                self.val_losses.append(learn.metrics.all_metrics['loss'].compute())
                self.mbar.update_graph([[fc.L.range(self.losses), self.losses],[fc.L.range(learn.epoch+1).map(lambda x: (x+1)*len(learn.dls.train)), self.val_losses]])

# %% ../nbs/learner.ipynb 49
from torch.optim.lr_scheduler import ExponentialLR


# %% ../nbs/learner.ipynb 50
class MomentumLearner(TrainLearner):
    def __init__(self,model,dls,loss_func,epochs,lr,cbs,opt_func = optim.SGD,mom=0.85):
        self.mom = mom
        super().__init__(model,dls,loss_func,epochs,lr,cbs,opt_func)
    def zero_grad(self):
        with torch.no_grad():
            for p in self.model.parameters(): p.grad *= self.mom

# %% ../nbs/learner.ipynb 51
class LR_find(Callback):
    def __init__(self,gamma = 1.3,max_mult=3):
        fc.store_attr()
    def before_fit(self,learn):
        self.sched = ExponentialLR(learn.opt,self.gamma)
        self.losses,self.lrs = [],[]
        self.min = math.inf
    def after_batch(self,learn):
        if not learn.training: raise CancelEpochException()
        self.lrs.append(learn.opt.param_groups[0]['lr'])
        loss = to_cpu(learn.loss)
        self.losses.append(loss)
        if loss < self.min: self.min = loss
        if math.isnan(loss) or loss > self.min * self.max_mult : raise CancelFitException()
        
        self.sched.step()
    def plot(self):
        plt.plot(self.lrs,self.losses)
        
        
        

        
        

# %% ../nbs/learner.ipynb 55
@fc.patch
def lr_find(self:Learner, gamma=1.3, max_mult=3, start_lr=1e-5, max_epochs=10):
    self.fit(max_epochs, lr=start_lr, cbs=LRFinderCB(gamma=gamma, max_mult=max_mult))
